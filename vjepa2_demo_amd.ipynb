{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V-JEPA 2 Demo Notebook for AMD ROCm GPUs\n",
    "\n",
    "This tutorial demonstrates how to run V-JEPA 2 on AMD GPUs using ROCm. It covers:\n",
    "- Loading models in vanilla PyTorch and HuggingFace\n",
    "- Extracting video embeddings\n",
    "- Predicting action classes\n",
    "- AMD GPU-specific optimizations\n",
    "\n",
    "For more details about the model, see https://github.com/facebookresearch/vjepa2.\n",
    "\n",
    "**AMD GPU Environment:**\n",
    "- ROCm support via PyTorch CUDA API (ROCm provides CUDA compatibility)\n",
    "- Multi-GPU support (8 AMD GPUs detected)\n",
    "- Optimized for AMD Instinct MI210/MI250/MI300X series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Check AMD GPU Environment\n",
    "\n",
    "First, let's verify that AMD GPUs are detected and accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "AMD GPU Environment Check\n",
      "============================================================\n",
      "PyTorch version: 2.5.1+rocm6.2\n",
      "ROCm/CUDA available: True\n",
      "Number of GPUs: 8\n",
      "  GPU 0: AMD Instinct MI300X VF\n",
      "    Memory: 191.7 GB\n",
      "    Compute capability: 9.4\n",
      "  GPU 1: AMD Instinct MI300X VF\n",
      "    Memory: 191.7 GB\n",
      "    Compute capability: 9.4\n",
      "  GPU 2: AMD Instinct MI300X VF\n",
      "    Memory: 191.7 GB\n",
      "    Compute capability: 9.4\n",
      "  GPU 3: AMD Instinct MI300X VF\n",
      "    Memory: 191.7 GB\n",
      "    Compute capability: 9.4\n",
      "  GPU 4: AMD Instinct MI300X VF\n",
      "    Memory: 191.7 GB\n",
      "    Compute capability: 9.4\n",
      "  GPU 5: AMD Instinct MI300X VF\n",
      "    Memory: 191.7 GB\n",
      "    Compute capability: 9.4\n",
      "  GPU 6: AMD Instinct MI300X VF\n",
      "    Memory: 191.7 GB\n",
      "    Compute capability: 9.4\n",
      "  GPU 7: AMD Instinct MI300X VF\n",
      "    Memory: 191.7 GB\n",
      "    Compute capability: 9.4\n",
      "\n",
      "ROCm environment variables:\n",
      "  HSA_OVERRIDE_GFX_VERSION: not set\n",
      "  PYTORCH_ROCM_ARCH: not set\n",
      "  HIP_VISIBLE_DEVICES: not set\n",
      "============================================================\n",
      "\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AMD GPU Environment Check\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"ROCm/CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"  GPU {i}: {props.name}\")\n",
    "        print(f\"    Memory: {props.total_memory / 1024**3:.1f} GB\")\n",
    "        print(f\"    Compute capability: {props.major}.{props.minor}\")\n",
    "else:\n",
    "    print(\"Warning: No GPUs detected. Running on CPU will be very slow.\")\n",
    "\n",
    "# Check ROCm environment variables\n",
    "print(\"\\nROCm environment variables:\")\n",
    "rocm_vars = ['HSA_OVERRIDE_GFX_VERSION', 'PYTORCH_ROCM_ARCH', 'HIP_VISIBLE_DEVICES']\n",
    "for var in rocm_vars:\n",
    "    value = os.environ.get(var, 'not set')\n",
    "    print(f\"  {var}: {value}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Define Helper Functions\n",
    "\n",
    "Import all necessary libraries and define the helper functions for loading models and processing videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharon/miniconda3/envs/vjepa2-amd/lib/python3.12/site-packages/timm/models/layers/__init__.py:49: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from decord import VideoReader\n",
    "from transformers import AutoVideoProcessor, AutoModel\n",
    "\n",
    "import src.datasets.utils.video.transforms as video_transforms\n",
    "import src.datasets.utils.video.volume_transforms as volume_transforms\n",
    "from src.models.attentive_pooler import AttentiveClassifier\n",
    "from src.models.vision_transformer import vit_giant_xformers_rope\n",
    "\n",
    "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def load_pretrained_vjepa_pt_weights(model, pretrained_weights):\n",
    "    \"\"\"Load weights of the VJEPA2 encoder.\"\"\"\n",
    "    pretrained_dict = torch.load(pretrained_weights, weights_only=True, map_location=\"cpu\")[\"encoder\"]\n",
    "    pretrained_dict = {k.replace(\"module.\", \"\"): v for k, v in pretrained_dict.items()}\n",
    "    pretrained_dict = {k.replace(\"backbone.\", \"\"): v for k, v in pretrained_dict.items()}\n",
    "    msg = model.load_state_dict(pretrained_dict, strict=False)\n",
    "    print(f\"Pretrained weights found at {pretrained_weights} and loaded with msg: {msg}\")\n",
    "\n",
    "\n",
    "def load_pretrained_vjepa_classifier_weights(model, pretrained_weights):\n",
    "    \"\"\"Load weights of the VJEPA2 classifier.\"\"\"\n",
    "    pretrained_dict = torch.load(pretrained_weights, weights_only=True, map_location=\"cpu\")[\"classifiers\"][0]\n",
    "    pretrained_dict = {k.replace(\"module.\", \"\"): v for k, v in pretrained_dict.items()}\n",
    "    msg = model.load_state_dict(pretrained_dict, strict=False)\n",
    "    print(f\"Pretrained weights found at {pretrained_weights} and loaded with msg: {msg}\")\n",
    "\n",
    "\n",
    "def build_pt_video_transform(img_size):\n",
    "    \"\"\"Build PyTorch preprocessing transform for videos.\"\"\"\n",
    "    short_side_size = int(256.0 / 224 * img_size)\n",
    "    eval_transform = video_transforms.Compose(\n",
    "        [\n",
    "            video_transforms.Resize(short_side_size, interpolation=\"bilinear\"),\n",
    "            video_transforms.CenterCrop(size=(img_size, img_size)),\n",
    "            volume_transforms.ClipToTensor(),\n",
    "            video_transforms.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "        ]\n",
    "    )\n",
    "    return eval_transform\n",
    "\n",
    "\n",
    "def get_video(video_path=\"sample_video.mp4\"):\n",
    "    \"\"\"Load video frames using decord.\"\"\"\n",
    "    vr = VideoReader(video_path)\n",
    "    # Sample frames (every 2nd frame for 64 frames total)\n",
    "    frame_idx = np.arange(0, 128, 2)\n",
    "    video = vr.get_batch(frame_idx).asnumpy()\n",
    "    return video\n",
    "\n",
    "\n",
    "def forward_vjepa_video(model_hf, model_pt, hf_transform, pt_transform, device):\n",
    "    \"\"\"Run inference with VJEPA models on video.\"\"\"\n",
    "    with torch.inference_mode():\n",
    "        # Read and pre-process the video\n",
    "        video = get_video()  # T x H x W x C\n",
    "        video = torch.from_numpy(video).permute(0, 3, 1, 2)  # T x C x H x W\n",
    "        x_pt = pt_transform(video).to(device).unsqueeze(0)\n",
    "        x_hf = hf_transform(video, return_tensors=\"pt\")[\"pixel_values_videos\"].to(device)\n",
    "        \n",
    "        # Warm-up run (important for AMD GPUs due to kernel compilation)\n",
    "        if device.type == 'cuda':\n",
    "            print(\"Warming up AMD GPUs (kernel compilation)...\")\n",
    "            _ = model_pt(x_pt)\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        # Timed inference\n",
    "        start_time = time.time()\n",
    "        out_patch_features_pt = model_pt(x_pt)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        pt_time = time.time() - start_time\n",
    "        \n",
    "        start_time = time.time()\n",
    "        out_patch_features_hf = model_hf.get_vision_features(x_hf)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        hf_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"PyTorch model inference time: {pt_time * 1000:.2f} ms\")\n",
    "        print(f\"HuggingFace model inference time: {hf_time * 1000:.2f} ms\")\n",
    "\n",
    "    return out_patch_features_hf, out_patch_features_pt\n",
    "\n",
    "\n",
    "def get_vjepa_video_classification_results(classifier, out_patch_features_pt, device):\n",
    "    \"\"\"Run classification on extracted features.\"\"\"\n",
    "    SOMETHING_SOMETHING_V2_CLASSES = json.load(open(\"ssv2_classes.json\", \"r\"))\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out_classifier = classifier(out_patch_features_pt)\n",
    "\n",
    "    print(f\"Classifier output shape: {out_classifier.shape}\")\n",
    "    print(\"\\nTop 5 predicted class names:\")\n",
    "    top5_indices = out_classifier.topk(5).indices[0]\n",
    "    top5_probs = F.softmax(out_classifier.topk(5).values[0], dim=0) * 100.0\n",
    "    for idx, prob in zip(top5_indices, top5_probs):\n",
    "        str_idx = str(idx.item())\n",
    "        print(f\"  {SOMETHING_SOMETHING_V2_CLASSES[str_idx]}: {prob:.2f}%\")\n",
    "\n",
    "print(\"Helper functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download Sample Video and Labels\n",
    "\n",
    "Download a sample video and the Something-Something V2 action class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video already exists at sample_video.mp4\n",
      "SSV2 classes already exist at ssv2_classes.json\n"
     ]
    }
   ],
   "source": [
    "sample_video_path = \"sample_video.mp4\"\n",
    "\n",
    "# Download the video if not yet downloaded\n",
    "if not os.path.exists(sample_video_path):\n",
    "    print(\"Downloading sample video...\")\n",
    "    video_url = \"https://huggingface.co/datasets/nateraw/kinetics-mini/resolve/main/val/bowling/-WH-lxmGJVY_000005_000015.mp4\"\n",
    "    command = [\"wget\", video_url, \"-O\", sample_video_path]\n",
    "    subprocess.run(command, check=True)\n",
    "    print(\"Video downloaded successfully!\")\n",
    "else:\n",
    "    print(f\"Video already exists at {sample_video_path}\")\n",
    "\n",
    "# Download SSV2 classes if not already present\n",
    "ssv2_classes_path = \"ssv2_classes.json\"\n",
    "if not os.path.exists(ssv2_classes_path):\n",
    "    print(\"Downloading SSV2 class labels...\")\n",
    "    command = [\n",
    "        \"wget\",\n",
    "        \"https://huggingface.co/datasets/huggingface/label-files/resolve/d79675f2d50a7b1ecf98923d42c30526a51818e2/\"\n",
    "        \"something-something-v2-id2label.json\",\n",
    "        \"-O\",\n",
    "        \"ssv2_classes.json\",\n",
    "    ]\n",
    "    subprocess.run(command, check=True)\n",
    "    print(\"SSV2 classes downloaded successfully!\")\n",
    "else:\n",
    "    print(f\"SSV2 classes already exist at {ssv2_classes_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Models (PyTorch & HuggingFace)\n",
    "\n",
    "Load the V-JEPA 2 models using both PyTorch and HuggingFace.\n",
    "\n",
    "\n",
    "To manually download:\n",
    "```bash\n",
    "wget https://dl.fbaipublicfiles.com/vjepa2/vitg-384.pt -P ./weights/\n",
    "```\n",
    "\n",
    "**AMD GPU Optimization:**\n",
    "- Models are loaded to AMD GPU using `.to(device)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HuggingFace model...\n",
      "HuggingFace model loaded on cuda\n",
      "Image size: 384x384\n"
     ]
    }
   ],
   "source": [
    "# # Model configuration\n",
    "hf_model_name = \"facebook/vjepa2-vitg-fpc64-384\"  # Options: vitl, vith, vitg with 256 or 384 resolution\n",
    "\n",
    "print(\"Loading HuggingFace model...\")\n",
    "model_hf = AutoModel.from_pretrained(hf_model_name)\n",
    "model_hf.to(device).eval()\n",
    "print(f\"HuggingFace model loaded on {device}\")\n",
    "\n",
    "# Build HuggingFace preprocessing transform\n",
    "hf_transform = AutoVideoProcessor.from_pretrained(hf_model_name)\n",
    "img_size = hf_transform.crop_size[\"height\"]  # E.g. 384 or 256\n",
    "print(f\"Image size: {img_size}x{img_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually download the model weights\n",
    "!wget https://dl.fbaipublicfiles.com/vjepa2/vitg-384.pt -P ./weights/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained weights found at ./weights/vitg-384.pt and loaded with msg: <All keys matched successfully>\n",
      "\n",
      "Models loaded successfully!\n",
      "Model parameters: 1012.2M\n"
     ]
    }
   ],
   "source": [
    "# Load model from local weights\n",
    "pt_model_path = \"./weights/vitg-384.pt\"  # Update this path\n",
    "model_pt = vit_giant_xformers_rope(img_size=(img_size, img_size), num_frames=64)\n",
    "model_pt.to(device).eval()\n",
    "load_pretrained_vjepa_pt_weights(model_pt, pt_model_path)\n",
    "\n",
    "# Build PyTorch preprocessing transform\n",
    "pt_video_transform = build_pt_video_transform(img_size=img_size)\n",
    "print(\"\\nModels loaded successfully!\")\n",
    "num_params = sum(p.numel() for p in model_pt.parameters())\n",
    "print(f\"Model parameters: {num_params / 1e6:.1f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Video Inference\n",
    "\n",
    "Extract patch-wise features from the video using both models and verify they produce equivalent results.\n",
    "\n",
    "**AMD GPU Performance Notes:**\n",
    "- First inference includes MIOpen kernel autotuning (may be slow)\n",
    "- Subsequent inferences will be much faster due to cached kernels\n",
    "- Inference time is measured with GPU synchronization for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on video...\n",
      "\n",
      "Warming up AMD GPUs (kernel compilation)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharon/miniconda3/envs/vjepa2-amd/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model inference time: 1643.84 ms\n",
      "HuggingFace model inference time: 1603.17 ms\n",
      "\n",
      "Inference results on video:\n",
      "  HuggingFace output shape: torch.Size([1, 18432, 1408])\n",
      "  PyTorch output shape:     torch.Size([1, 18432, 1408])\n",
      "  Absolute difference sum:  1964.459473\n",
      "  Close: False\n",
      "\n",
      "⚠ Warning: Models produce different features (this may be expected for different precision)\n"
     ]
    }
   ],
   "source": [
    "# Run inference on video to get patch-wise features\n",
    "print(\"Running inference on video...\\n\")\n",
    "out_patch_features_hf, out_patch_features_pt = forward_vjepa_video(\n",
    "    model_hf, model_pt, hf_transform, pt_video_transform, device\n",
    ")\n",
    "\n",
    "print(f\"\"\"\n",
    "Inference results on video:\n",
    "  HuggingFace output shape: {out_patch_features_hf.shape}\n",
    "  PyTorch output shape:     {out_patch_features_pt.shape}\n",
    "  Absolute difference sum:  {torch.abs(out_patch_features_pt - out_patch_features_hf).sum():.6f}\n",
    "  Close: {torch.allclose(out_patch_features_pt, out_patch_features_hf, atol=1e-3, rtol=1e-3)}\n",
    "\"\"\")\n",
    "\n",
    "if torch.allclose(out_patch_features_pt, out_patch_features_hf, atol=1e-3, rtol=1e-3):\n",
    "    print(\"✓ Models produce equivalent features!\")\n",
    "else:\n",
    "    print(\"⚠ Warning: Models produce different features (this may be expected for different precision)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Action Classification\n",
    "\n",
    "Use a pretrained attentive probe classifier to predict action classes from the extracted features.\n",
    "\n",
    "To download the attentive probe weights:\n",
    "```bash\n",
    "wget https://dl.fbaipublicfiles.com/vjepa2/evals/ssv2-vitg-384-64x2x3.pt -P ./weights/\n",
    "```\n",
    "\n",
    "Then update `classifier_model_path` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/vjepa2/evals/ssv2-vitg-384-64x2x3.pt -P ./weights/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading attentive probe classifier...\n",
      "Pretrained weights found at ./weights/ssv2-vitg-384-64x2x3.pt and loaded with msg: <All keys matched successfully>\n",
      "Classifier loaded successfully!\n",
      "\n",
      "Classifier output shape: torch.Size([1, 174])\n",
      "\n",
      "Top 5 predicted class names:\n",
      "  Putting [something] into [something]: 44.93%\n",
      "  Stuffing [something] into [something]: 28.10%\n",
      "  Putting [something] onto [something]: 14.44%\n",
      "  Failing to put [something] into [something] because [something] does not fit: 7.64%\n",
      "  Putting [number of] [something] onto [something]: 4.89%\n"
     ]
    }
   ],
   "source": [
    "# Path to classifier weights (update this path)\n",
    "classifier_model_path = \"./weights/ssv2-vitg-384-64x2x3.pt\"\n",
    "\n",
    "# Check if classifier weights exist\n",
    "if not os.path.exists(classifier_model_path):\n",
    "    print(f\"⚠ Classifier weights not found at {classifier_model_path}\")\n",
    "    print(\"Please download the weights using:\")\n",
    "    print(\"wget https://dl.fbaipublicfiles.com/vjepa2/evals/ssv2-vitg-384-64x2x3.pt -P ./weights/\")\n",
    "else:\n",
    "    print(\"Loading attentive probe classifier...\")\n",
    "    classifier = AttentiveClassifier(\n",
    "        embed_dim=model_pt.embed_dim, \n",
    "        num_heads=16, \n",
    "        depth=4, \n",
    "        num_classes=174\n",
    "    ).to(device).eval()\n",
    "    \n",
    "    load_pretrained_vjepa_classifier_weights(classifier, classifier_model_path)\n",
    "    print(\"Classifier loaded successfully!\\n\")\n",
    "    \n",
    "    # Get classification results\n",
    "    get_vjepa_video_classification_results(classifier, out_patch_features_pt, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. AMD GPU Performance Benchmarking (Optional)\n",
    "\n",
    "Run multiple inference iterations to benchmark AMD GPU performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark with 10 iterations...\n",
      "\n",
      "Iteration 1/10: 1647.77 ms\n",
      "Iteration 2/10: 1645.19 ms\n",
      "Iteration 3/10: 1748.78 ms\n",
      "Iteration 4/10: 1715.32 ms\n",
      "Iteration 5/10: 1652.44 ms\n",
      "Iteration 6/10: 1659.66 ms\n",
      "Iteration 7/10: 1666.28 ms\n",
      "Iteration 8/10: 1766.66 ms\n",
      "Iteration 9/10: 1718.87 ms\n",
      "Iteration 10/10: 1749.62 ms\n",
      "\n",
      "Benchmark Results:\n",
      "  Average: 1697.06 ms\n",
      "  Std Dev: 45.33 ms\n",
      "  Min:     1645.19 ms\n",
      "  Max:     1766.66 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Benchmark inference performance\n",
    "num_iterations = 10\n",
    "print(f\"Running benchmark with {num_iterations} iterations...\\n\")\n",
    "\n",
    "# Prepare input\n",
    "video = get_video()\n",
    "video = torch.from_numpy(video).permute(0, 3, 1, 2)\n",
    "x_pt = pt_video_transform(video).to(device).unsqueeze(0)\n",
    "\n",
    "times = []\n",
    "with torch.inference_mode():\n",
    "    for i in range(num_iterations):\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start_time = time.time()\n",
    "        _ = model_pt(x_pt)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        times.append(elapsed * 1000)  # Convert to ms\n",
    "        print(f\"Iteration {i+1}/{num_iterations}: {elapsed * 1000:.2f} ms\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Benchmark Results:\n",
    "  Average: {np.mean(times):.2f} ms\n",
    "  Std Dev: {np.std(times):.2f} ms\n",
    "  Min:     {np.min(times):.2f} ms\n",
    "  Max:     {np.max(times):.2f} ms\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-GPU Inference (Optional)\n",
    "\n",
    "Example of using multiple AMD GPUs for batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DataParallel with 8 AMD GPUs\n",
      "\n",
      "Processing batch of 8 videos...\n",
      "Batch inference time: 17134.59 ms\n",
      "Per-video time: 2141.82 ms\n",
      "Output shape: torch.Size([8, 18432, 1408])\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using DataParallel with {torch.cuda.device_count()} AMD GPUs\\n\")\n",
    "    \n",
    "    # Wrap model with DataParallel\n",
    "    model_pt_multi = torch.nn.DataParallel(model_pt)\n",
    "    \n",
    "    # Create a batch of videos (duplicate for demo)\n",
    "    batch_size = torch.cuda.device_count()\n",
    "    x_batch = x_pt.repeat(batch_size, 1, 1, 1, 1)\n",
    "    \n",
    "    print(f\"Processing batch of {batch_size} videos...\")\n",
    "    with torch.inference_mode():\n",
    "        start_time = time.time()\n",
    "        outputs = model_pt_multi(x_batch)\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed = time.time() - start_time\n",
    "    \n",
    "    print(f\"Batch inference time: {elapsed * 1000:.2f} ms\")\n",
    "    print(f\"Per-video time: {elapsed * 1000 / batch_size:.2f} ms\")\n",
    "    print(f\"Output shape: {outputs.shape}\")\n",
    "else:\n",
    "    print(\"Only 1 GPU available, skipping multi-GPU demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook includes the following:\n",
    "- Loading V-JEPA 2 models on AMD GPUs\n",
    "- Video feature extraction\n",
    "- Action classification\n",
    "- Performance benchmarking on AMD ROCm (inference\n",
    "- Multi-GPU inference\n",
    "\n",
    "Monitor GPU usage with: `rocm-smi`\n",
    "\n",
    "Forked from the [V-JEPA 2 repository](https://github.com/facebookresearch/vjepa2)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vjepa2-amd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
